<!DOCTYPE html>
<html lang="en"><head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
<!-- Bootstrap CSS -->
    <link rel="stylesheet" href="/shared/IDEAS-developments/template-columbia/_assets/thirdpartylib/bootstrap-4.3.1/css/bootstrap.min.css">
    <!-- Font Awesome CSS -->
    <link rel="stylesheet" href="/shared/IDEAS-developments/template-columbia/_assets/thirdpartylib/fontawesome-free-5.9.0-web/css/all.min.css">
    <!-- Template CSS -->
    <link rel="stylesheet" href="/shared/IDEAS-developments/template-columbia/_assets/css/styles.min.css">
    <link rel="stylesheet" href="/shared/IDEAS-developments/template-columbia/_assets/css/custom.css">

    <!-- Able Player Dependencies-->
    <script src="//ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <script src="/shared/videos/ableplayer/thirdparty/js.cookie.js"></script>
    <link rel="stylesheet" href="/shared/videos/ableplayer/build/ableplayer.min.css" type="text/css">
    <script src="/shared/videos/ableplayer/build/ableplayer.min.js"></script>
    <script src="https://player.vimeo.com/api/player.js"></script>
    <script src="https://cdnapisec.kaltura.com/p/823192/sp/82319200/embedIframeJs/uiconf_id/46847663/partner_id/823192"></script>
    <script src="/shared/videos/ableplayer/ablefier.js"></script>
    <style>
          h3 {
			  font-style: italic;
		  }
		  pre {
			  font-weight: bold;
		  }
    </style>
<title>Lesson 4: Floating Point Rounding</title>
</head><body><div class="container-fluid">
<div class="row">
<div class="col-12 banner-img">
<p><img src="img/banner.jpg" alt="banner"></p>
</div>
<div class="col-sm-10 offset-sm-1">
<h1>Lesson 4: Floating Point Rounding</h1>
<p>Consider a single precision 32-bit IEEE-754 floating-point number. With 32 bits available, we have 2<sup>32</sup> possible bit configurations. Think of these 2<sup>32</sup> bit configurations as "breadcrumbs" that can be placed on the real number line from -∞ to +∞. We can view the IEEE-754 floating-point standard as a system for placing those "breadcrumbs" on the real number line.</p>
<img src="img/Floating%20Point%20Rounding_html_3b479acbf8bf8919.png" alt="Diagram of the real number line" title="The number line" style="max-width: 100%; display: block; margin-left: auto; margin-right: auto;" width="472" height="169">
<p>(Note that the breadcrumbs are not actually evenly spaced, as this diagram might suggest. The IEEE-754 standard places breadcrumbs that are closer to zero closer together and breadcrumbs that are further from zero further apart.)</p>
<p>Any floating-point calculation is likely to produce a result that, mathematically speaking, would want to fall between two adjacent breadcrumbs. However, the computer can only produce results consisting of bit configurations, so it will have to choose one of the two breadcrumbs that surround the mathematical result. Because of this, the results produced by the computer will almost always be mathematically wrong (for this not to be the case, the mathematical result would have to land exactly on top of a breadcrumb), and this difference between the mathematical result and what the computer can actually produce is referred to as <b>roundoff error</b>.</p>
<p>In fact, IEEE 754 <i>requires</i> that the hardware compute the correctly rounded result. Internally, the floating-point unit uses extra precision bits (called <b>guard</b>, <b>round</b>, and <b>sticky</b> bits) to determine which breadcrumb is closest to the exact mathematical result, even though the exact result itself is not representable. The default rounding mode always picks the nearest breadcrumb. But there are situations where the mathematical result falls <i>exactly</i> halfway between two breadcrumbs, and the hardware needs a tie-breaking rule. This is where different <b>rounding modes</b> come in.</p>
<p>The floating-point unit supports four rounding modes:</p>
<ul>
<li>
<p><b>towards +∞</b>: always pick the breadcrumb that is closest to +∞</p>
</li>
<li>
<p><b>towards -∞</b>: always pick the breadcrumb that is closest to -∞</p>
</li>
<li>
<p><b>towards 0</b>: always pick the breadcrumb that is closest to 0</p>
</li>
<li>
<p><b>towards even (default)</b>: pick the nearest breadcrumb; when exactly at the midpoint, pick the breadcrumb whose rightmost bit is 0</p>
</li>
</ul>
<p>The <b>towards even</b> rounding mode is the default. Some explanation of it is necessary here. As you go along the number line in either direction, breadcrumbs whose rightmost bit is 0 always alternate with breadcrumbs whose rightmost bit is 1.</p>
<img src="img/Floating%20Point%20Rounding_html_a1f65e3b2a6bc246.png" alt="Even Odd Diagram" title="Even Odd Diagram" style="max-width: 100%; display: block; margin-left: auto; margin-right: auto;">
<p>Therefore, any mathematical result will <i>always</i> fall between a breadcrumb whose rightmost bit is 0 and one whose rightmost bit is 1. The idea behind this rounding mode is that sometimes a mathematical result will be rounded up towards +&infin; and sometimes down towards &minus;&infin;. The hope is that upwards and downwards roundoff errors will cancel each other out, producing a more accurate result. Clearly, this is only a hope, and that hope may not always come true.</p>

<h2>Concrete Rounding Examples in Decimal</h2>
<p>Before looking at binary, let us build intuition by rounding decimal numbers to one decimal place. Suppose the exact result of some computation is 2.35 and we must round to one decimal place. The two nearest representable values are 2.3 and 2.4, and 2.35 falls <i>exactly</i> at the midpoint.</p>
<ul>
<li><b>Toward +&infin;:</b> 2.4 (round up toward positive infinity)</li>
<li><b>Toward &minus;&infin;:</b> 2.3 (round down toward negative infinity)</li>
<li><b>Toward 0:</b> 2.3 (round toward zero, which is downward for a positive number)</li>
<li><b>Toward even:</b> 2.4 (the digit in the last kept place would be 4, which is even)</li>
</ul>
<p>Now consider 2.45 rounded to one decimal place. The two nearest values are 2.4 and 2.5, and 2.45 is exactly at the midpoint.</p>
<ul>
<li><b>Toward +&infin;:</b> 2.5</li>
<li><b>Toward &minus;&infin;:</b> 2.4</li>
<li><b>Toward 0:</b> 2.4</li>
<li><b>Toward even:</b> 2.4 (the digit in the last kept place is 4, which is even &mdash; so we round down)</li>
</ul>
<p>Notice that round-to-even only differs from the other modes when the value is <i>exactly</i> at the midpoint between two representable values. When the value is clearly closer to one side, all four modes agree (except toward +&infin; and toward &minus;&infin; may differ in direction).</p>

<h2>Binary Rounding Examples</h2>
<p>Now let us look at rounding in binary, which is what the hardware actually does. Suppose our floating point format has <b>3 fraction bits</b> (so the significand looks like 1.xxx). We need to round values that have more than 3 fraction bits down to exactly 3.</p>

<h3>Below the midpoint: round down</h3>
<p>Consider the value 1.00101. This falls between the representable values 1.001 and 1.010. The dropped bits are <b>01</b>, which is below the midpoint (the midpoint would be 10). All rounding modes round toward the closer value:</p>
<pre>    1.00101 &rarr; 1.001   (rounded down; the dropped bits 01 are below the midpoint)</pre>

<h3>Above the midpoint: round up</h3>
<p>Consider the value 1.00111. Again between 1.001 and 1.010. The dropped bits are <b>11</b>, which is above the midpoint. All modes round toward the closer value:</p>
<pre>    1.00111 &rarr; 1.010   (rounded up; the dropped bits 11 are above the midpoint)</pre>

<h3>Exactly at the midpoint: round-to-even decides</h3>
<p>Consider the value 1.00110. The dropped bits are <b>10</b>, which is <i>exactly</i> the midpoint between 1.001 and 1.010. Now round-to-even looks at the last kept bit:</p>
<pre>    1.001|10  &rarr;  last kept bit is 1 (odd), so round up to make it 0 (even)
    Result: 1.010</pre>

<p>Now consider 1.01010. This is exactly at the midpoint between 1.010 and 1.011. The dropped bits are <b>10</b>:</p>
<pre>    1.010|10  &rarr;  last kept bit is 0 (even), so round down to keep it even
    Result: 1.010</pre>

<p>In both midpoint cases, round-to-even chose the result whose last fraction bit is 0. This is the key rule: <b>at an exact midpoint, choose the value whose least significant kept bit is 0</b>.</p>

<h2>Guard, Round, and Sticky Bits</h2>
<p>When the floating point hardware computes an arithmetic result (such as an addition or multiplication), the exact result typically has more bits than fit in the fraction field. The hardware cannot keep all of these extra bits, but it keeps three extra bits beyond the fraction field to determine the correct rounding direction:</p>
<ul>
<li><b>Guard bit (G):</b> the first bit beyond the fraction field</li>
<li><b>Round bit (R):</b> the second bit beyond the fraction field</li>
<li><b>Sticky bit (S):</b> the logical OR of <i>all</i> remaining bits beyond the round bit</li>
</ul>
<p>The sticky bit is crucial: it tells us whether there is <i>any</i> nonzero information beyond the first two extra bits, without needing to store all those bits individually.</p>
<p>These three bits determine the rounding direction under round-to-even:</p>
<pre>
    GRS = 0xx  &rarr;  below midpoint  &rarr;  round down (truncate)
    GRS = 100  &rarr;  exactly at midpoint  &rarr;  round to even
    GRS = 101, 110, or 111  &rarr;  above midpoint  &rarr;  round up
</pre>

<h3>Example: Adding with guard, round, and sticky bits</h3>
<p>Suppose we have a 3-fraction-bit system and want to compute 1.001 + 0.0101. First, align the exponents by shifting the smaller number right:</p>
<pre>
      1.001  (this is 1.001 &times; 2<sup>0</sup>)
    + 0.0101 (this is 1.01 &times; 2<sup>&minus;2</sup>, shifted right by 2)
    --------
      1.0111
</pre>
<p>The exact result is 1.0111. With 3 fraction bits, we keep 1.011 and the extra bits are: G=1, R=0, S=0. This is GRS = 100 &mdash; exactly at the midpoint. The last kept bit is 1 (odd), so round-to-even rounds up:</p>
<pre>    1.011 + 0.001 = 1.100</pre>
<p>The correctly rounded result is 1.100.</p>

<h2>Why Round-to-Even Is the Default</h2>
<p>Consider what happens if we always round <i>up</i> at the midpoint (as many people learn in grade school). Over many operations, every midpoint case adds a small positive error. These errors <b>accumulate</b>, introducing a systematic upward bias. After thousands of operations, the result can be significantly too high.</p>
<p>Similarly, always rounding toward zero introduces a systematic bias that makes positive results too small and negative results too large (in magnitude, too small).</p>
<p>Round-to-even is <b>statistically unbiased</b>: at midpoint cases, it rounds up half the time (when the last kept bit is odd) and rounds down half the time (when the last kept bit is already even). Over many operations, the positive and negative rounding errors tend to cancel out, producing more accurate accumulated results. This is why IEEE 754 chose it as the default.</p>
<p>The other three modes &mdash; toward +&infin;, toward &minus;&infin;, and toward 0 &mdash; still exist because they are useful for <b>interval arithmetic</b>, where a programmer computes both an upper bound (rounding toward +&infin;) and a lower bound (rounding toward &minus;&infin;) to determine the range of possible results. This technique is used in scientific computing to rigorously bound the error in a computation. In C, the rounding mode can be changed at runtime using the <code>fesetround()</code> function from <code>&lt;fenv.h&gt;</code>.</p>

<h2>Practical Impact: Accumulated Rounding Error</h2>
<p>Here is a simple C program that demonstrates how rounding errors accumulate:</p>
<pre>
    float sum = 0.0f;
    for (int i = 0; i &lt; 10000000; i++) {
        sum += 0.1f;
    }
    printf("Expected: 1000000.0, Got: %f\n", sum);
</pre>
<p>The output will <b>not</b> be 1000000.0. It will be something like 1000000.125 or a similar value, depending on the system. The reason is twofold: first, 0.1 cannot be represented exactly in binary (as we saw in Lesson 3), so every addition introduces a small error; second, as <code>sum</code> grows large, the small value 0.1f loses more and more of its significant bits when aligned to <code>sum</code>'s exponent, compounding the error.</p>
<p>This is not a bug in the hardware &mdash; it is the inevitable consequence of finite precision arithmetic. Understanding rounding is essential for writing software that produces reliable numerical results.</p>

<p>Computers must necessarily represent numbers as bits. But <i>bits are not numbers</i>, and getting bits to act like numbers cannot be perfectly accomplished. Computer scientists have gotten <i>close</i>, but perfection is impossible. It's necessary for any competent programmer to understand the ways in which bits <i>do not</i> act like numbers and to make sure that this does not cause incorrect results in the software they write.</p>
<footer>End of Lesson 4: Floating Point Rounding</footer></div>
</div>
</div></body></html>